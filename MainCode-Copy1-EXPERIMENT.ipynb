{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Udacity DRL Project2 - Continuous Control: PPO Solution\n",
    "#### Adapted from:\n",
    "- Shangtong Zhang: https://github.com/ShangtongZhang/DeepRL\n",
    "- Jeremi Kaczmarczyk: https://github.com/jknthn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from network_model import PPONet\n",
    "from ppo_agent import PPOAgent\n",
    "from ppo_agentEXP import PPOAgentEXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.vector_action_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file\n",
    "config = {\n",
    "    'environment': {\n",
    "        'state_size':  env_info.vector_observations.shape[1],\n",
    "        'action_size': brain.vector_action_space_size,\n",
    "        'number_of_agents': len(env_info.agents)\n",
    "    },\n",
    "    'pytorch': {\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'discount_rate': 0.99,\n",
    "        'tau': 0.95,\n",
    "        'gradient_clip': 5,\n",
    "        'rollout_length': 2048,\n",
    "        'optimization_epochs': 10,\n",
    "        'ppo_clip': 0.2,\n",
    "        'log_interval': 2048,\n",
    "        'max_steps': 1e5,\n",
    "        'mini_batch_number': 32,\n",
    "        'entropy_coefficent': 0.01,\n",
    "        'episode_count': 250,\n",
    "        'hidden_size': 512,\n",
    "        'adam_learning_rate': 3e-4,\n",
    "        'adam_epsilon': 1e-5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = PPONet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPONet = PPONet(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.forward(states) #.to.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute an iteration of:\n",
    "- reset env\n",
    "- fetch current state of system\n",
    "- select action \n",
    "- send action to env \n",
    "- get next state \n",
    "- get reward \n",
    "- chech if done \n",
    "- update state \n",
    "- exit if finished\n",
    "\"\"\"\n",
    "\n",
    "def step_iteration(env, brain_name, policy, config):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]                       # reset the environment   \n",
    "    states = env_info.vector_observations                                   # get current state\n",
    "    scores = np.zeros(config['environment']['number_of_agents'])            # initialize scores\n",
    "    while True:                                                             # Loop until done\n",
    "        actions, _, _, _ = policy(states)                                   # select an action via the policy\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]     # send the action to the env\n",
    "        next_states = env_info.vector_observations                          # get the next state\n",
    "        rewards = env_info.rewards                                          # get the reward\n",
    "        dones = env_info.local_done                                         # see if episode has finished\n",
    "        scores += env_info.rewards                                          # update the scores with the reward\n",
    "        states = next_states                                                # update state with next_state\n",
    "        if np.any(dones):                                                   # exit loop if episode is finished\n",
    "            break\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo_episodes(env, brain_name, policy, config, train):\n",
    "    if train:\n",
    "        optimizier = optim.Adam(policy.parameters(), config['hyperparameters']['adam_learning_rate'], \n",
    "                        eps=config['hyperparameters']['adam_epsilon'])\n",
    "        #agent = PPOAgent(env, brain_name, policy, optimizier, config)\n",
    "        agent = PPOAgentEXP(env, brain_name, policy, optimizier, config)\n",
    "        all_scores = []\n",
    "        averages = []\n",
    "        last_max = 30.0\n",
    "        for i in tqdm.tqdm(range(config['hyperparameters']['episode_count'])):   # do number of episodes\n",
    "            agent.step()                                                         # perform step function inside PPOAgent\n",
    "            last_mean_reward = step_iteration(env, brain_name, policy, config)   # step/loop thru an episode until \"done\"\n",
    "            last_100_average = np.mean(np.array(all_scores[-100:])) if len(all_scores) > 100 else np.mean(np.array(all_scores))\n",
    "            all_scores.append(last_mean_reward)\n",
    "            averages.append(last_100_average)\n",
    "            if last_100_average > last_max:\n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model\n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended\n",
    "                #torch.save(policy.state_dict(), f\"models/ppo-max-hiddensize-{config['hyperparameters']['hidden_size']}.pth\")\n",
    "                torch.save(policy.state_dict(), f\"BestModel.pth\")\n",
    "                last_max = last_100_average\n",
    "            clear_output(True)\n",
    "            print('Episode: {} Total score this episode: {} Last {} average: {}'.format(i + 1, last_mean_reward, min(i + 1, 100), last_100_average))\n",
    "        return all_scores, averages\n",
    "    else:\n",
    "        # step/loop thru an episode until \"done\"\n",
    "        score = step_iteration(env, brain_name, policy, config)\n",
    "        return [score], [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------- \n",
    "#### Train the PPO network with Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "new_policy = PPONet(config)\n",
    "all_scores, average_scores = run_ppo_episodes(env, brain_name, new_policy, config, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "plt.ylabel('All Scores')\n",
    "plt.xlabel('Episode #')\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "plt.plot(np.arange(len(average_scores)), average_scores)\n",
    "plt.ylabel('Average 100 Scores')\n",
    "plt.xlabel('Episode #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFER\n",
    "policy = PPONet(config)\n",
    "#policy.load_state_dict(torch.load('models/ppo-max-hiddensize-512.pth'))\n",
    "policy.load_state_dict(torch.load('BestModel.pth'))\n",
    "_, _ = run_ppo_episodes(env, brain_name, policy, config, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizier = optim.Adam(policy.parameters(), config['hyperparameters']['adam_learning_rate'], \n",
    "#                        eps=config['hyperparameters']['adam_epsilon'])\n",
    "#agent = PPOAgent(env, brain_name, policy, optimizier, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play_round(env, brain_name, policy, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, _, _, _ = policy(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.forward(states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
